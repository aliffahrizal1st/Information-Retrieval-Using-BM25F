{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Temu Kembali Sinopsis Film.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qt2p7Ldf1vzJ"
      },
      "source": [
        "# **INSTALL LIBRARY**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSxlfX6z1Ybd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "609d100f-c004-463d-d2f3-7418fa443abb"
      },
      "source": [
        "pip install Sastrawi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Sastrawi in /usr/local/lib/python3.7/dist-packages (1.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhe_vL6M16c3"
      },
      "source": [
        "# **IMPORT LIBRARY**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGvrsnJS02oG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6b89fac-9002-4ac6-fb70-19c8ae77bba0"
      },
      "source": [
        "import pandas as pd\n",
        "import re \n",
        "import math \n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from google.colab import files, drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i934V6QK4Iz_"
      },
      "source": [
        "# **PREPROCESSING**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6g2rl-vZ5h-"
      },
      "source": [
        "**Case Folding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrVMdtTmY5LA"
      },
      "source": [
        "def caseFolding(text):\n",
        "  text = [i.lower() for i in text]\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYDCA0DvYsxu"
      },
      "source": [
        "**Tokenisasi (Tokenization)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-wx8hyxcVaN"
      },
      "source": [
        "def tokenisasi(text):\n",
        "  text = [re.sub(r'([0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)', ' ', i) for i in text]\n",
        "  text = [i.split(' ') for i in text]\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAUM-fFXYutt"
      },
      "source": [
        "**Filtering**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iu9LUospdRpZ"
      },
      "source": [
        "def filtering(stopwords,text):\n",
        "  with open(stopwords) as f:\n",
        "    content = f.readlines()\n",
        "  sw = [word.strip() for word in content]\n",
        "  \n",
        "  fltr=[]\n",
        "  for i in range(len(text)):\n",
        "    result=[]\n",
        "    for j in range(len(text[i])):\n",
        "      if not text[i][j] in sw:\n",
        "        result.append(text[i][j])\n",
        "    fltr.append(result)\n",
        "  \n",
        "  fltr = [\" \".join(word) for word in fltr]\n",
        "  return fltr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faKOSKVfYwSF"
      },
      "source": [
        "**Stemming**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIqwg7hvmIul"
      },
      "source": [
        "def stemmingProcess(fltr):\n",
        "  stem = []\n",
        "  factory = StemmerFactory()\n",
        "  stemmer = factory.create_stemmer()\n",
        "  \n",
        "  for i in range(len(fltr)):\n",
        "    hasilstem = stemmer.stem(fltr[i])\n",
        "    stem.append(hasilstem)\n",
        "    \n",
        "  return stem"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TopsYEgKoszx"
      },
      "source": [
        "**Term Unik (Extract Unique Term)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tS-FKxPnotMj"
      },
      "source": [
        "def uniqueTerm(text,hasil=[]):\n",
        "  for i in text:\n",
        "    for j in i.split(): \n",
        "      if j not in hasil:\n",
        "        hasil.append(j)\n",
        "  return hasil"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWSml5UnqROi"
      },
      "source": [
        "**Term Frequency**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0HE5rsmqhsE"
      },
      "source": [
        "def termFrequency(text,feature):\n",
        "  hasil = []\n",
        "  text = [i.split() for i in text]\n",
        "  for i in range(len(text)):\n",
        "    hasil.append([])\n",
        "    for j in range(len(feature)):\n",
        "      hasil[i].append(text[i].count(feature[j]))\n",
        "  return hasil"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeUvf6qm9wWk"
      },
      "source": [
        "**Document Frequency**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVElB9xPC9H2"
      },
      "source": [
        "def doctFrequency(text,feature):\n",
        "  df = []\n",
        "  text = [i.split() for i in text]\n",
        "  for i in feature:\n",
        "    jml = 0\n",
        "    for j in text:\n",
        "      if i in j:\n",
        "        jml+=1\n",
        "    df.append(jml)\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1ORdRi6wMbG"
      },
      "source": [
        "# **BM25F Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaW-tJFr9Rcv"
      },
      "source": [
        "***idf(t)***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjFP3mTp9RFl"
      },
      "source": [
        "def idfProcess(dfResult,nDoc = 0):\n",
        "  idfResult = []\n",
        "  for i in dfResult:\n",
        "    idfResult.append(math.log((nDoc-i+0.5)/(i+0.5),10))\n",
        "  return idfResult"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTFmcIKnJnld"
      },
      "source": [
        "**lc & avlc**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHORr_7gDNdv"
      },
      "source": [
        "def lcProcess(doc):\n",
        "  splitedDoc = [i.split() for i in doc]\n",
        "  total_lc = 0\n",
        "  each_lc = []\n",
        "  for i in splitedDoc:\n",
        "    each_lc.append(len(i))\n",
        "    total_lc+= len(i)\n",
        "  avg_lc = total_lc / len(splitedDoc)\n",
        "  return each_lc, avg_lc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZAGpHsLKgAn"
      },
      "source": [
        "**ùë§ùëíùëñùëî‚Ñéùë°(ùë°,ùëë)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKKNqUB2Kfwm"
      },
      "source": [
        "def weight_td(titleQuery,contenctQuery,boost_title,boost_content,bc,total_lc_title, avg_lc_title,total_lc_content, avg_lc_content):\n",
        "  weighted_result = []\n",
        "  for i in range(len(titleQuery)):\n",
        "    weighted_result.append([])\n",
        "    for j in range(len(titleQuery[i])):\n",
        "      title = (titleQuery[i][j] * boost_title) / ((1 - bc) + bc * (total_lc_title[i] / avg_lc_title))\n",
        "      content = (contenctQuery[i][j] * boost_content) / ((1 - bc) + bc * (total_lc_content[i] / avg_lc_content))\n",
        "      result = title + content\n",
        "      weighted_result[i].append(result)\n",
        "  return weighted_result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9U1yPdFaT_uU"
      },
      "source": [
        "**Rangking ùëπ(ùíí,ùíÖ)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3GwjaOrUIGB"
      },
      "source": [
        "#BM25F\n",
        "def rangking(idf,k1,weighted):\n",
        "  rank = []\n",
        "  for i in range(len(weighted)):\n",
        "    sum = 0\n",
        "    for j in range(len(weighted[i])):\n",
        "      sum += idf[j] * (weighted[i][j] / (k1+weighted[i][j]))\n",
        "    rank.append([sum,'D'+str(i+1)])\n",
        "  return rank"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABlrvcKF9PKN"
      },
      "source": [
        "# **Main Code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Upf9LHdlY5iU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f2a3b93-3cba-4423-f22f-5b207b73c4f0"
      },
      "source": [
        "#1. Read Dataset\n",
        "df = pd.read_excel('...')\n",
        "stopwords = '...'\n",
        "query = '...'\n",
        "\n",
        "#2. Input Parameter\n",
        "boost_title = 5\n",
        "boost_content = 2\n",
        "bc = 0.75\n",
        "k1 = 1.2\n",
        "\n",
        "#3. Split Between Title & The Content\n",
        "title = df['Judul'].values.tolist()\n",
        "content = df['Isi'].values.tolist()\n",
        "\n",
        "#4.1 Preprocessing Both title & content\n",
        "#4.1.1 Case Folding\n",
        "title_cf = caseFolding(title)\n",
        "content_cf = caseFolding(content)\n",
        "#4.1.2 Tokenisasi\n",
        "title_token = tokenisasi(title_cf)\n",
        "content_token = tokenisasi(content_cf)\n",
        "#4.1.3 Filtering\n",
        "title_filter = filtering(stopwords,title_token)\n",
        "content_filter = filtering(stopwords,content_token)\n",
        "#4.1.4 Stemming\n",
        "title_stemming = stemmingProcess(title_filter)\n",
        "content_stemming = stemmingProcess(content_filter)\n",
        "#4.1.5 Unique Term\n",
        "#Only contain unique term on title\n",
        "title_uniqueTerm = uniqueTerm(title_stemming)\n",
        "#Contain unique term both on title & content\n",
        "content_uniqueTerm = uniqueTerm(content_stemming,title_uniqueTerm)\n",
        "#4.1.6 Term Frequency\n",
        "title_tf = termFrequency(title_stemming,content_uniqueTerm)\n",
        "content_tf = termFrequency(content_stemming,content_uniqueTerm)\n",
        "\n",
        "#4.2 Preprocessing the query\n",
        "#4.2.1 Case Folding\n",
        "cfQuery = caseFolding([query])\n",
        "#4.2.2 Tokenisasi\n",
        "tQuery = tokenisasi(cfQuery)\n",
        "#4.2.3 Filtering\n",
        "fQuery = filtering(stopwords,tQuery)\n",
        "#4.2.4 Stemming\n",
        "sQuery = stemmingProcess(fQuery)\n",
        "#4.2.5 Unique Term\n",
        "uniqueQuery = uniqueTerm(sQuery,[])\n",
        "#4.2.6 Term Frequency\n",
        "titleQuery = termFrequency(title_stemming,uniqueQuery)\n",
        "contenctQuery = termFrequency(content_stemming,uniqueQuery)\n",
        "\n",
        "#5. BM25F Model\n",
        "#5.1 Document Frequency (df) process\n",
        "dfContent = doctFrequency(content_stemming,uniqueQuery)\n",
        "\n",
        "#5.2 Inverese Document Frequency (idf) process\n",
        "idf = idfProcess(dfContent,len(content))\n",
        "\n",
        "#5.3 Count lc & avlc\n",
        "total_lc_title, avg_lc_title = lcProcess(title_stemming)\n",
        "total_lc_content, avg_lc_content = lcProcess(content_stemming)\n",
        "\n",
        "#5.4 Count weighted(t,d)\n",
        "weighted = weight_td(titleQuery,contenctQuery,boost_title,boost_content,bc,total_lc_title, avg_lc_title,total_lc_content, avg_lc_content)\n",
        "\n",
        "#5.5 Count R(q,D) process\n",
        "rank = rangking(idf,k1,weighted)\n",
        "print(rank)\n",
        "\n",
        "#5.6 Sorting rank by value\n",
        "rank.sort(key=lambda x:x[0], reverse=True)\n",
        "print(rank)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.9922150350082405, 'D1'], [0.22026048122479064, 'D2'], [0.09275772989523397, 'D3'], [0.1280222143923306, 'D4'], [0.0, 'D5']]\n",
            "[[0.9922150350082405, 'D1'], [0.22026048122479064, 'D2'], [0.1280222143923306, 'D4'], [0.09275772989523397, 'D3'], [0.0, 'D5']]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}